{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting text from multi-column pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/akshat/miniconda3/envs/mindmaps/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /home/akshat/miniconda3/envs/mindmaps/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/akshat/miniconda3/envs/mindmaps/lib/python3.10/site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: click in /home/akshat/miniconda3/envs/mindmaps/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/akshat/miniconda3/envs/mindmaps/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/akshat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/akshat/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "DIGITIZED_FILE = \"Zanzibar.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def fonts(doc, granularity=False):\n",
    "    \"\"\"Extracts fonts and their usage in PDF documents.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param granularity: also use 'font', 'flags' and 'color' to discriminate text\n",
    "    :type granularity: bool\n",
    "    :rtype: [(font_size, count), (font_size, count}], dict\n",
    "    :return: most used fonts sorted by count, font style information\n",
    "    \"\"\"\n",
    "    styles = {}\n",
    "    font_counts = {}\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # block contains text\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if granularity:\n",
    "                            identifier = \"{0}_{1}_{2}_{3}\".format(s['size'], s['flags'], s['font'], s['color'])\n",
    "                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],\n",
    "                                                  'color': s['color']}\n",
    "                        else:\n",
    "                            identifier = \"{0}\".format(s['size'])\n",
    "                            styles[identifier] = {'size': s['size'], 'font': s['font']}\n",
    "\n",
    "                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage\n",
    "\n",
    "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if len(font_counts) < 1:\n",
    "        raise ValueError(\"Zero discriminating fonts found!\")\n",
    "\n",
    "    return font_counts, styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_tags(font_counts, styles):\n",
    "    \"\"\"Returns dictionary with font sizes as keys and tags as value.\n",
    "    :param font_counts: (font_size, count) for all fonts occuring in document\n",
    "    :type font_counts: list\n",
    "    :param styles: all styles found in the document\n",
    "    :type styles: dict\n",
    "    :rtype: dict\n",
    "    :return: all element tags based on font-sizes\n",
    "    \"\"\"\n",
    "    p_style = styles[font_counts[0][0]]  # get style for most used font by count (paragraph)\n",
    "    p_size = p_style['size']  # get the paragraph's size\n",
    "\n",
    "    # sorting the font sizes high to low, so that we can append the right integer to each tag \n",
    "    font_sizes = []\n",
    "    for (font_size, count) in font_counts:\n",
    "        font_sizes.append(float(font_size))\n",
    "    font_sizes.sort(reverse=True)\n",
    "\n",
    "    # aggregating the tags for each font size\n",
    "    idx = 0\n",
    "    size_tag = {}\n",
    "    for size in font_sizes:\n",
    "        idx += 1\n",
    "        if size == p_size:\n",
    "            idx = 0\n",
    "            size_tag[size] = '<p>'\n",
    "        if size > p_size:\n",
    "            size_tag[size] = '<h{0}>'.format(idx)\n",
    "        elif size < p_size:\n",
    "            size_tag[size] = '<s{0}>'.format(idx)\n",
    "\n",
    "    return size_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_para(doc, size_tag):\n",
    "    \"\"\"Scrapes headers & paragraphs from PDF and return texts with element tags.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param size_tag: textual element tags for each size\n",
    "    :type size_tag: dict\n",
    "    :rtype: list\n",
    "    :return: texts with pre-prended element tags\n",
    "    \"\"\"\n",
    "    header_para = []  # list with headers and paragraphs\n",
    "    first = True  # boolean operator for first header\n",
    "    previous_s = {}  # previous span\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # this block contains text\n",
    "\n",
    "                # REMEMBER: multiple fonts and sizes are possible IN one block\n",
    "\n",
    "                block_string = \"\"  # text found in block\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if s['text'].strip():  # removing whitespaces:\n",
    "                            if first:\n",
    "                                previous_s = s\n",
    "                                first = False\n",
    "                                block_string = size_tag[s['size']] + s['text']\n",
    "                            else:\n",
    "                                if s['size'] == previous_s['size']:\n",
    "\n",
    "                                    if block_string and all((c == \"|\") for c in block_string):\n",
    "                                        # block_string only contains pipes\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    if block_string == \"\":\n",
    "                                        # new block has started, so append size tag\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    else:  # in the same block, so concatenate strings\n",
    "                                        block_string += \" \" + s['text']\n",
    "\n",
    "                                else:\n",
    "                                    header_para.append(block_string)\n",
    "                                    block_string = size_tag[s['size']] + s['text']\n",
    "\n",
    "                                previous_s = s\n",
    "\n",
    "                    # new block started, indicating with a pipe\n",
    "                    # block_string += \"|\"\n",
    "\n",
    "                header_para.append(block_string)\n",
    "\n",
    "    return header_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(DIGITIZED_FILE)\n",
    "font_style, styles = fonts(doc)\n",
    "size_tag = font_tags(font_style, styles)\n",
    "headers_para = headers_para(doc, size_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = []\n",
    "paragraphs = []\n",
    "for text in headers_para:\n",
    "    if text.startswith(\"<h\"):\n",
    "        headings.append(text[3:])\n",
    "    elif text.startswith(\"<s\"):\n",
    "        pass\n",
    "    else:\n",
    "        if text.endswith(\"|\"):\n",
    "            paragraphs.append(text[3:-1])\n",
    "        else:\n",
    "            paragraphs.append(text[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    " \n",
    "class LemmatizerHelper(object):\n",
    "    \"\"\"\n",
    "    Class to aid the lemmatization process - from word to stemmed form,\n",
    "    and vice versa.\n",
    "    The 'original' form of a lemmatized word will be returned as the\n",
    "    form in which its been used the most number of times in the text.\n",
    "    \"\"\"\n",
    " \n",
    "    #This reverse lookup will remember the original forms of the lemmatized\n",
    "    #words\n",
    "    word_lookup = {}\n",
    " \n",
    "    @classmethod\n",
    "    def lemmatize(cls, sentence):\n",
    "        \"\"\"\n",
    "        Lemmatize a sentence and updates the reverse lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        #Lemmatize the word\n",
    "        lemmatized = [wnl.lemmatize(word) for word in sentence.split()]\n",
    "        lemmatized = \" \".join(lemmatized)\n",
    " \n",
    "        return lemmatized\n",
    " \n",
    "    @classmethod\n",
    "    def original_form(cls, word):\n",
    "        \"\"\"\n",
    "        Returns original form of a word given the lemmatized version,\n",
    "        as stored in the word lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        if word in cls.word_lookup:\n",
    "            return max(cls.word_lookup[word].keys(),\n",
    "                       key=lambda x: cls.word_lookup[word][x])\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Determining whether online user are authorized to access digital object is central to preserving privacy. This pa- per present the design, implementation, and deployment of Zanzibar, a global system for storing and evaluating ac- ce control lists. Zanzibar provides a uniform data model and configuration language for expressing a wide range of access control policy from hundred of client service at Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Its authorization decision respect causal or- dering of user action and thus provide external consistency amid change to access control list and object contents. Zanzibar scale to trillion of access control list and million of authorization request per second to support service used by billion of people. It ha maintained 95th-percentile la- tency of le than 10 millisecond and availability of greater than 99.999% over 3 year of production use.', 'Many online interaction require authorization check to confirm that a user ha permission to carry out an operation on a digital object. For example, web-based photo storage service typically allow photo owner to share some photo with friend while keeping other photo private. Such a ser- vice must check whether a photo ha been shared with a user before allowing that user to view the photo. Robust autho- rization check are central to preserving online privacy. This paper present Zanzibar, a system for storing per- mission and performing authorization check based on the stored permissions. It is used by a wide array of service offered by Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Several of these service manage bil- lion of object on behalf of more than a billion users. A unified authorization system offer important advan- tages over maintaining separate access control mechanism for individual applications. First, it help establish consistent', 'semantics and user experience across applications. Second, it make it easier for application to interoperate, for exam- ple, to coordinate access control when an object from one ap- plication embeds an object from another application. Third, useful common infrastructure can be built on top of a unified access control system, in particular, a search index that re- spects access control and work across applications. Finally, a we show below, authorization pose unique challenge in- volving data consistency and scalability. It save engineering resource to tackle them once across applications.', 'We have the following goal for the Zanzibar system:', '• Correctness : It must ensure consistency of access con- trol decision to respect user intentions.', '• Flexibility : It must support a rich set of access control policy a required by both consumer and enterprise applications.', '• Low latency : It must respond quickly because autho- rization check are often in the critical path of user in- teractions. Low latency at the tail is particularly impor- tant for serving search results, which often require ten to hundred of checks.', '• High availability : It must reliably respond to request because, in the absence of explicit authorizations, client service would be forced to deny their user access.', '• Large scale : It need to protect billion of object shared by billion of users. It must be deployed around the globe to be near it client and their end users.', 'Zanzibar achieves these goal through a combination of notable features. To provide flexibility, Zanzibar pair a sim- ple data model with a powerful configuration language. The language allows client to define arbitrary relation between user and objects, such a owner , editor , commenter , and viewer . It includes set-algebraic operator such a inter- section and union for specifying potentially complex access control policy in term of those user-object relations. For example, an application can specify that user granted edit- ing right on a document are also allowed to comment on the', 'document, but not all commenters are given editing rights. At runtime, Zanzibar allows client to create, modify, and evaluate access control list (ACLs) through a remote proce- dure call (RPC) interface. A simple ACL take the form of “user U ha relation R to object O ”. More complex ACLs take the form of “set of user S ha relation R to object O ”, where S is itself specified in term of another object-relation pair. ACLs can thus refer to other ACLs, for example to specify that the set of user who can comment on a video consists of the user who have been granted viewing right on that specific video along with those with viewing permis- sion on the video channel. Group membership are an important class of ACL where the object is a group and the relation is semantically equiv- alent to member . Groups can contain other groups, which illustrates one of the challenge facing Zanzibar, namely that evaluating whether a user belongs to a group can entail fol- lowing a long chain of nested group memberships. Authorization check take the form of “does user U have relation R to object O ?” and are evaluated by a collection of distributed servers. When a check request arrives to Zanz- ibar, the work to evaluate the check may fan out to multiple servers, for example when a group contains both individual member and other groups. Each of those server may in turn contact other servers, for example to recursively traverse a hierarchy of group memberships. Zanzibar operates at a global scale along multiple dimen- sions. It store more than two trillion ACLs and performs million of authorization check per second. The ACL data doe not lend itself to geographic partitioning because au- thorization check for any object can come from anywhere in the world. Therefore, Zanzibar replicates all ACL data in ten of geographically distributed data center and distributes load across thousand of server around the world. Zanzibar support global consistency of access control de- cisions through two interrelated features. One, it respect the order in which ACL change are committed to the underlying data store. Two, it can ensure that authorization check are based on ACL data no older than a client-specified change. Thus, for example, a client can remove a user from a group and be assured that subsequent membership check reflect that removal. Zanzibar provides these ordering property by storing ACLs in a globally distributed database system with external consistency guarantee [15, 18]. Zanzibar employ an array of technique to achieve low latency and high availability in this globally distributed en- vironment. Its consistency protocol allows the vast majority of request to be served with locally replicated data, with- out requiring cross-region round trips. Zanzibar store it data in normalized form for consistency. It handle hot spot on normalized data by caching final and intermediate results, and by deduplicating simultaneous requests. It also applies technique such a hedging request and optimizing computation on deeply nested set with limited denormal-', 'ization. Zanzibar responds to more than 95% of authoriza- tion check within 10 millisecond and ha maintained more than 99.999% availability for the last 3 years. The main contribution of this paper lie in conveying the engineering challenge in building and deploying a consis- tent, world-scale authorization system. While most element of Zanzibar’s design have their root in previous research, this paper provides a record of the feature and technique Zanzibar brings together to satisfy it stringent requirement for correctness, flexibility, latency, availability, and scalabil- ity. The paper also highlight lesson learned from operating Zanzibar in service of a diverse set of demanding clients.', 'This section describes Zanzibar’s data model, configuration language, and application programming interface (API).', 'In Zanzibar, ACLs are collection of object-user or object- object relation represented a relation tuples . Groups are simply ACLs with membership semantics. Relation tuples have efficient binary encodings, but in this paper we repre- sent them using a convenient text notation:', '⟨ tuple ⟩ ::= ⟨ object ⟩ ‘ # ’ ⟨ relation ⟩ ‘ @ ’ ⟨ user ⟩', '⟨ object ⟩ ::= ⟨ namespace ⟩ ‘ : ’ ⟨ object id ⟩', '⟨ user ⟩ ::= ⟨ user id ⟩ | ⟨ userset ⟩', '⟨ userset ⟩ ::= ⟨ object ⟩ ‘ # ’ ⟨ relation ⟩', 'where ⟨ namespace ⟩ and ⟨ relation ⟩ are predefined in client configuration (§2.3), ⟨ object id ⟩ is a string, and ⟨ user id ⟩ is an integer. The primary key required to identify a relation tuple are ⟨ namespace ⟩ , ⟨ object id ⟩ , ⟨ relation ⟩ , and ⟨ user ⟩ . One feature worth noting is that a ⟨ userset ⟩ allows ACLs to refer to group and thus support representing nested group membership. Table 1 show some example tuples and corresponding se- mantics. While some relation (e.g. viewer ) define access control directly, others (e.g. parent , pointing to a folder) only define abstract relation between objects. These ab- stract relation may indirectly affect access control given userset rewrite rule specified in namespace configs (§2.3.1). Defining our data model around tuples, instead of per- object ACLs, allows u to unify the concept of ACLs and group and to support efficient read and incremental up- dates, a we will see in §2.4.', 'ACL check must respect the order in which user modify ACLs and object content to avoid unexpected sharing be- haviors. Specifically, our client care about preventing the', 'Example Tuple in Text Notation Semantics', 'doc:readme#owner@10 User 10 is an owner of doc:readme', 'group:eng#member@11 User 11 is a member of group:eng', 'doc:readme#viewer@group:eng#member Members of group:eng are viewer of doc:readme', 'doc:readme#parent@folder:A#... doc:readme is in folder:A', 'Table 1: Example relation tuples. “#...” represents a relation that doe not affect the semantics of the tuple.', '“new enemy” problem, which can arise when we fail to re- spect the ordering between ACL update or when we apply old ACLs to new content. Consider these two examples:', 'Example A: Neglecting ACL update order', '1. Alice remove Bob from the ACL of a folder; 2. Alice then asks Charlie to move new document to the folder, where document ACLs inherit from folder ACLs;', '3. Bob should not be able to see the new documents, but may do so if the ACL check neglect the ordering between the two ACL changes.', 'Example B: Misapplying old ACL to new content', '1. Alice remove Bob from the ACL of a document; 2. Alice then asks Charlie to add new content to the document;', '3. Bob should not be able to see the new contents, but may do so if the ACL check is evaluated with a stale ACL from before Bob’s removal.', 'Preventing the “new enemy” problem requires Zanzibar to understand and respect the causal ordering between ACL or content updates, including update on different ACLs or ob- jects and those coordinated via channel invisible to Zanz- ibar. Hence Zanzibar must provide two key consistency properties: external consistency [18] and snapshot read with bounded staleness . External consistency allows Zanzibar to assign a times- tamp to each ACL or content update, such that two causally related update x ≺ y will be assigned timestamps that reflect the causal order: T', '< T', '. With causally meaningful times- tamps, a snapshot read of the ACL database at timestamp T , which observes all update with timestamps ≤ T , will re- spect ordering between ACL updates. That is, if the read observes an update x , it will observe all update that happen causally before x . Furthermore, to avoid applying old ACLs to new contents, the ACL check evaluation snapshot must not be staler than the causal timestamp assigned to the content update. Given a content update at timestamp T', ', a snapshot read at timestamp', '≥ T', 'ensures that all ACL update that happen causally be- fore the content update will be observed by the ACL check. To provide external consistency and snapshot read with bounded staleness, we store ACLs in the Spanner global database system [15]. Spanner’s TrueTime mechanism as- sign each ACL write a microsecond-resolution timestamp, such that the timestamps of writes reflect the causal ordering between writes, and thereby provide external consistency. We evaluate each ACL check at a single snapshot timestamp across multiple database reads, so that all writes with times- tamp up to the check snapshot, and only those writes, are visible to the ACL check. To avoid evaluating check for new content using stale ACLs, one could try to always evaluate at the latest snapshot such that the check result reflects all ACL writes up to the check call. However, such evaluation would require global data synchronization with high-latency round trip and lim- ited availability. Instead, we design the following protocol to allow most check to be evaluated on already replicated data with cooperation from Zanzibar clients:', '1. A Zanzibar client request an opaque consistency token called a zookie for each content version, via a content- change ACL check (§2.4.4) when the content modifi- cation is about to be saved. Zanzibar encodes a current global timestamp in the zookie and ensures that all prior ACL writes have lower timestamps. The client store the zookie with the content change in an atomic write to the client storage. Note that the content-change check doe not need to be evaluated in the same transaction a the application content modification, but only ha to be triggered when the user modifies the contents.', '2. The client sends this zookie in subsequent ACL check request to ensure that the check snapshot is at least a fresh a the timestamp for the content version.', 'External consistency and snapshot read with staleness bounded by zookie prevent the “new enemy” problem. In Example A, ACL update A 1 and A 2 will be assigned times- tamp T', '< T', ', respectively. Bob will not be able to see the new document added by Charlie: if a check is evalu- ated at T < T', ', the document ACLs will not include the folder ACL; if a check is evaluated at T ≥ T', '> T', ', the check will observe update A 1, which removed Bob from the', 'folder ACL. In Example B, Bob will not see the new content added to the document. For Bob to see the new contents, the check must be evaluated with a zookie ≥ T', ', the timestamp assigned to the content update. Because T', '> T', ', such a check will also observe the ACL update B 1, which removed Bob from the ACL. The zookie protocol is a key feature of Zanzibar’s consis- tency model. It ensures that Zanzibar respect causal order- ing between ACL and content updates, but otherwise grant Zanzibar freedom to choose evaluation timestamps so a to meet it latency and availability goals. The freedom arises from the protocol’s at-least-as-fresh semantics, which allow Zanzibar to choose any timestamp fresher than the one en- coded in a zookie. Such freedom in turn allows Zanzibar to serve most check at a default staleness with already repli- cated data (§3.2.1) and to quantize evaluation timestamps to avoid hot spot (§3.2.5).', 'Before client can store relation tuples in Zanzibar, they must configure their namespaces. A namespace configura- tion specifies it relation a well a it storage parameters. Each relation ha a name, which is a client-defined string such a viewer or editor , and a relation config. Storage pa- rameters include sharding setting and an encoding for object IDs that help Zanzibar optimize storage of integer, string, and other object ID formats.', '2.3.1 Relation Configs and Userset Rewrites', 'While relation tuples reflect relationship between object and users, they do not completely define the effective ACLs. For example, some client specify that user with editor permission on each object should have viewer permission on the same object. While such relationship between rela- tions can be represented by a relation tuple per object, storing a tuple for each object in a namespace would be wasteful and make it hard to make modification across all objects. In- stead, we let client define object-agnostic relationship via userset rewrite rule in relation configs. Figure 1 demon- strates a simple namespace configuration with concentric re- lations, where viewer contains editor , and editor con- tains owner . Userset rewrite rule are defined per relation in a names- pace. Each rule specifies a function that take an object ID a input and output a userset expression tree. Each leaf node of the tree can be any of the following:', '• this : Returns all user from stored relation tuples for the ⟨ object # relation ⟩ pair, including indirect ACLs ref- erenced by usersets from the tuples. This is the default behavior when no rewrite rule is specified.', '• computed userset : Computes, for the input object, a new userset. For example, this allows the userset ex- pression for a viewer relation to refer to the editor userset on the same object, thus offering an ACL inher-', 'Figure 1: Simple namespace configuration with concentric relation on documents. All owner are editors, and all ed- itors are viewers. Further, viewer of the parent folder are also viewer of the document.', 'itance capability between relations.', '• tuple to userset : Computes a tupleset (§2.4.1) from the input object, fetch relation tuples matching the tupleset, and computes a userset from every fetched relation tuple. This flexible primitive allows our client to express complex policy such a “look up the parent folder of the document and inherit it viewers”.', 'A userset expression can also be composed of multiple sub-expressions, combined by operation such a union, in- tersection, and exclusion.', 'In addition to supporting ACL checks, Zanzibar also pro- vides APIs for client to read and write relation tuples, watch tuple updates, and inspect the effective ACLs.', 'A concept used throughout these API method is that of a zookie . A zookie is an opaque byte sequence encoding a globally meaningful timestamp that reflects an ACL write, a client content version, or a read snapshot. Zookies in ACL read and check request specify staleness bound for snap- shot reads, thus providing one of Zanzibar’s core consistency properties. We choose to use an opaque cookie instead of the actual timestamp to discourage our client from choosing ar- bitrary timestamps and to allow future extensions.', '2.4.1 Read', 'Our client read relation tuples to display ACLs or group membership to users, or to prepare for a subsequent write. A read request specifies one or multiple tuplesets and an op- tional zookie. Each tupleset specifies key of a set of relation tuples. The set can include a single tuple key, or all tuples with a given object ID or userset in a namespace, optionally constrained by a relation name. With the tuplesets, client can look up a specific membership entry, read all entry in an ACL or group, or look up all group with a given user a a direct member. All tuplesets in a read request are processed at a single snapshot. With the zookie, client can request a read snapshot no earlier than a previous write if the zookie from the write re- sponse is given in the read request, or at the same snapshot a a previous read if the zookie from the earlier read response is given in the subsequent request. If the request doesn’t contain a zookie, Zanzibar will choose a reasonably recent snapshot, possibly offering a lower-latency response than if a zookie were provided. Read result only depend on content of relation tuples and do not reflect userset rewrite rules. For example, even if the viewer userset always includes the owner userset, reading tuples with the viewer relation will not return tuples with the owner relation. Clients that need to understand the ef- fective userset can use the Expand API (§2.4.5).', '2.4.2 Write', 'Clients may modify a single relation tuple to add or remove an ACL. They may also modify all tuples related to an object via a read-modify-write process with optimistic concurrency control [21] that us a read RPC followed by a write RPC:', '1. Read all relation tuples of an object, including a per- object “lock” tuple.', '2. Generate the tuples to write or delete. Send the writes, along with a touch on the lock tuple, to Zanzibar, with the condition that the writes will be committed only if the lock tuple ha not been modified since the read.', '3. If the write condition is not met, go back to step 1.', 'The lock tuple is just a regular relation tuple used by client to detect write races.', '2.4.3 Watch', 'Some client maintain secondary index of relation tuples in Zanzibar. They can do so with our Watch API. A watch request specifies one or more namespaces and a zookie rep- resenting the time to start watching. A watch response con- tains all tuple modification event in ascending timestamp order, from the requested start timestamp to a timestamp en- coded in a heartbeat zookie included in the watch response. The client can use the heartbeat zookie to resume watching where the previous watch response left off.', '2.4.4 Check', 'A check request specifies a userset, represented by ⟨ object # relation ⟩ , a putative user, often represented by an au- thentication token, and a zookie corresponding to the desired object version. Like reads, a check is always evaluated at a consistent snapshot no earlier than the given zookie. To authorize application content modifications, our client send a special type of check request, a content-change check. A content-change check request doe not carry a zookie and is evaluated at the latest snapshot. If a content change is authorized, the check response includes a zookie for client to store along with object content and use for subsequent check of the content version. The zookie encodes the evalu- ation snapshot and capture any possible causality from ACL change to content changes, because the zookie’s timestamp will be greater than that of the ACL update that protect the new content (§2.2).', '2.4.5 Expand', 'The Expand API return the effective userset given an ⟨ object # relation ⟩ pair and an optional zookie. Unlike the Read API, Expand follows indirect reference expressed through userset rewrite rules. The result is represented by a userset tree whose leaf node are user IDs or usersets pointing to other ⟨ object # relation ⟩ pairs, and intermediate node represent union, intersection, or exclusion operators. Expand is crucial for our client to reason about the com- plete set of user and group that have access to their ob- jects, which allows them to build efficient search index for access-controlled content.', 'Figure 2 show the architecture of the Zanzibar system. aclservers are the main server type. They are organized in cluster and respond to Check, Read, Expand, and Write requests. Requests arrive at any server in a cluster and that server fan out the work to other server in the cluster a necessary. Those server may in turn contact other server to compute intermediate results. The initial server gather the final result and return it to the client. Zanzibar store ACLs and their metadata in Spanner databases. There is one database to store relation tuples for each client namespace, one database to hold all namespace configurations, and one changelog database shared across all namespaces. aclservers read and write those database in the course of responding to client requests. watchservers are a specialized server type that respond to Watch requests. They tail the changelog and serve a stream of namespace change to client in near real time. Zanzibar periodically run a data processing pipeline to perform a variety of offline function across all Zanzibar data in Spanner. One such function is to produce dump of the re- lation tuples in each namespace at a known snapshot times-', 'Figure 2: Zanzibar architecture. Arrows indicate the direction of data flow.', 'tamp. Another is to garbage-collect tuple version older than a threshold configured per namespace. Leopard is an indexing system used to optimize operation on large and deeply nested sets. It read periodic snapshot of ACL data and watch for change between snapshots. It performs transformation on that data, such a denormaliza- tion, and responds to request from aclservers . The rest of this section present the implementation of these architectural element in more detail.', '3.1.1 Relation Tuple Storage', 'We store relation tuples of each namespace in a separate database, where each row is identified by primary key (shard ID, object ID, relation, user, commit timestamp) . Multi- ple tuple version are stored on different rows, so that we can evaluate check and read at any timestamp within the garbage collection window. The ordering of primary key allows u to look up all relation tuples for a given object ID or (object ID, relation) pair. Our client configure sharding of a namespace according to it data pattern. Usually the shard ID is determined solely by the object ID. In some cases, for example, when a names- pace store group with very large number of members, the shard ID is computed from both object ID and user.', '3.1.2 Changelog', 'Zanzibar also maintains a changelog database that store a history of tuple update for the Watch API. The primary key are (changelog shard ID, timestamp, unique update ID) , where a changelog shard is randomly selected for each write. Every Zanzibar write is committed to both the tuple stor-', 'age and the changelog shard in a single transaction. We des- ignate the Spanner server hosting the changelog shard a the transaction coordinator to minimize blocking of changelog read on pending transactions.', '3.1.3 Namespace Config Storage', 'Namespace configs are stored in a database with two ta- bles. One table contains the configs and is keyed by names- pace IDs. The other is a changelog of config update and is keyed by commit timestamps. This structure allows a Zanz- ibar server to load all configs upon startup and monitor the changelog to refresh configs continuously.', '3.1.4 Replication', 'To reduce latency, Zanzibar data is replicated to be close to our clients. Replicas exist in dozen of location around the world, with multiple replica per region. The 5 voting repli- ca are in eastern and central United States, in 3 different metropolitan area to isolate failure but within 25 millisec- onds of each other so that Paxos transaction commit quickly.', '3.2.1 Evaluation Timestamp', 'As noted in §2.4, client can provide zookies to ensure a minimum snapshot timestamp for request evaluation. When a zookie is not provided, the server us a default staleness chosen to ensure that all transaction are evaluated at a times- tamp that is a recent a possible without impacting latency. On each read request it make to Spanner, Zanzibar re- ceives a hint about whether or not the data at that timestamp required an out-of-zone read and thus incurred additional la- tency. Each server track the frequency of such out-of-zone read for data at a default staleness a well a for fresher', 'and staler data, and us these frequency to compute a bi- nomial proportion confidence interval of the probability that any given piece of data is available locally at each staleness. Upon collecting enough data, the server check to see if each staleness value ha a sufficiently low probability of in- curring an out-of-zone read, and thus will be low-latency. If so, it update the default staleness bound to the lowest “safe” value. If no known staleness value are safe, we use a two- proportion z -test to see if increasing the default will be a sta- tistically significant amount safer. In that case, we increase the default value in the hope of improving latency. This default staleness mechanism is purely a performance opti- mization. It doe not violate consistency semantics because Zanzibar always respect zookies when provided.', '3.2.2 Config Consistency', 'Because change to namespace configs can change the re- sults of ACL evaluations, and therefore their correctness, Zanzibar chooses a single snapshot timestamp for con- fig metadata when evaluating each client request. All aclservers in a cluster use that same timestamp for the same request, including for any subrequests that fan out from the original client request. Each server independently load namespace configs from storage continuously a they change (§3.1.3). Therefore, each server in a cluster may have access to a different range of config timestamps due to restarts or network latency. Zanzibar must pick a timestamp that is available across all of them. To facilitate this, a monitoring job track the times- tamp range available to every server and aggregate them, reporting a globally available range to every other server. On each incoming request the server pick a time from this range, ensuring that all server can continue serving even if they are no longer able to read from the config storage.', '3.2.3 Check Evaluation', 'Zanzibar evaluates ACL check by converting check request to boolean expressions. In a simple case, when there are no userset rewrite rules, checking a user U against a userset ⟨ object # relation ⟩ can be expressed a', '( U , ⟨ object # relation ⟩ ) =', '∃ tuple ⟨ object # relation @ U ⟩', '∨∃ tuple ⟨ object # relation @ U', '⟩ , where', 'U', '= ⟨ object', '# relation', '⟩ s.t.', '( U , U', ') .', 'Finding a valid U', '= ⟨ object', '# relation', '⟩ involves evaluat- ing membership on all indirect ACLs or groups, recursively. This kind of “pointer chasing” work well for most type of ACLs and groups, but can be expensive when indirect ACLs or group are deep or wide. §3.2.4 explains how we han- dle this problem. Userset rewrite rule are also translated to boolean expression a part of check evaluation. To minimize check latency, we evaluate all leaf node of the boolean expression tree concurrently. When the outcome', 'of one node determines the result of a subtree, evaluation of other node in the subtree is cancelled. Evaluation of leaf node usually involves reading relation tuples from databases. We apply a pooling mechanism to group read for the same ACL check to minimize the number of read RPCs to Spanner.', '3.2.4 Leopard Indexing System', 'Recursive pointer chasing during check evaluation ha diffi- culty maintaining low latency with group that are deeply nested or have a large number of child groups. For se- lected namespaces that exhibit such structure, Zanzibar han- dle check using Leopard, a specialized index that support efficient set computation. A Leopard index represents a collection of named set us- ing ( T , s , e ) tuples, where T is an enum representing the set type and s and e are 64-bit integer representing the set ID and the element ID, respectively. A query evaluates an ex- pression of union, intersection, or exclusion of named set and return the result set ordered by the element ID up to a specified number of results. To index and evaluate group membership, Zanzibar repre- sent group membership with two set types,', '2', 'and', '2', ', which we show here a function mapping from a set ID to element IDs:', '•', '2', '( s ) → { e } , where s represents an ances- tor group and e represents a descendent group that is directly or indirectly a sub-group of the ancestor group.', '•', '2', '( s ) → { e } , where s represents an in- dividual user and e represents a parent group in which the user is a direct member.', 'To evaluate whether user U is a member of group G , we check whether', '(', '2', '( U ) ∩', '2', '( G )) ̸ = /0', 'Group membership can be considered a a reachability problem in a graph, where node represent group and user and edge represent direct membership. Flattening group-to- group path allows reachability to be efficently evaluated by Leopard, though other type of denormalization can also be applied a data pattern demand. The Leopard system consists of three discrete parts: a serving system capable of consistent and low-latency oper- ations across sets; an offline, periodic index building system; and an online real-time layer capable of continuously updat- ing the serving system a tuple change occur. Index tuples are stored a ordered list of integer in a structure such a a skip list, thus allowing for efficient union and intersection among sets. For example, evaluat- ing the intersection between two sets, A and B , requires only O ( min ( | A | , | B | )) skip-list seeks. The index is sharded by el- ement IDs and can be distributed across multiple servers. Shards are usually served entirely from memory, but they', 'can also be served from a mix of hot and cold data spread between memory and remote solid-state devices. The offline index builder generates index shard from a snapshot of Zanzibar relation tuples and configs, and repli- cates the shard globally. It respect userset rewrite rule and recursively expands edge in an ACL graph to form Leop- ard index tuples. The Leopard server continously watch for new shard and swap old shard with new one when they become available. The Leopard system described thus far is able to effi- ciently evaluate deeply and widely nested group member- ship, but cannot do so at a fresh and consistent snapshot due to offline index generation and shard swapping. To sup- port consistent ACL evaluation, Leopard server maintain an incremental layer that index all update since the offline snapshot, where each update is represented by a ( T , s , e , t , d ) tuple, where t is the timestamp of the update and d is a dele- tion marker. Updates with timestamps le than or equal to the query timestamp are merged on top of the offline index during query processing. To maintain the incremental layer, the Leopard incremen- tal indexer call Zanzibar’s Watch API to receive a tem- porally ordered stream of Zanzibar tuple modification and transforms the update into a temporally ordered stream of Leopard tuple additions, updates, and deletions. Generat- ing update for the', '2', 'tuples requires the incre- mental indexer to maintain group-to-group membership for denormalizing the effect of a relation tuple update to poten- tially multiple index updates. In practice, a single Zanzibar tuple addition or deletion may yield potentially ten of thousand of discrete Leop- ard tuple events. Each Leopard serving instance receives the complete stream of these Zanzibar tuple change through the Watch API. The Leopard serving system is designed to con- tinuously ingest this stream and update it various posting list with minimal impact to query serving.', '3.2.5 Handling Hot Spots', 'The workload of ACL read and check is often bursty and subject to hot spots. For example, answering a search query requires conducting ACL check for all candidate results, whose ACLs often share common group or indirect ACLs. To facilitate consistency, Zanzibar avoids storage denormal- ization and relies only on normalized data (except for the case described in §3.2.4). With normalized data, hot spot on common ACLs (e.g., popular groups) may overload the underlying database servers. We found the handling of hot spot to be the most critical frontier in our pursuit of low latency and high availability. Zanzibar server in each cluster form a distributed cache for both read and check evaluations, including intermediate check result evaluated during pointer chasing. Cache en- try are distributed across Zanzibar server with consistent hashing [20]. To process check or reads, we fan out re-', 'quest to the corresponding Zanzibar server via an internal RPC interface. To minimize the number of internal RPCs, for most namespaces we compute the forwarding key from the object ID, since processing a check on ⟨ object # relation ⟩ often involves indirect ACL check on other relation of the same object and reading relation tuples of the object. These check and read can be processed by the same server since they share the same forwarding key with the parent check request. To handle hot forwarding keys, we cache result at both the caller and the callee of internal RPCs, effectively forming cache trees. We also use Slicer [12] to help dis- tribute hot key to multiple servers.', 'We avoid reusing result evaluated from a different snap- shot by encoding snapshot timestamps in cache keys. We choose evaluation timestamps rounded up to a coarse granu- larity, such a one or ten seconds, while respecting staleness constraint from request zookies. This timestamp quantiza- tion allows the vast majority of recent check and read to be evaluated at the same timestamps and to share cache results, despite having microsecond-resolution timestamps in cache keys. It is worth noting that rounding up timestamps doe not affect Zanzibar’s consistency properties, since Spanner ensures that a snapshot read at timestamp T will observe all writes up to T —this hold even if T is in the future, in which case the read will wait until TrueTime ha moved past T .', 'To handle the “cache stampede” problem [3], where con- current request create flash hot spot before the cache is populated with results, we maintain a lock table on each server to track outstanding read and checks. Among re- quest sharing the same cache key only one request will be- gin processing; the rest block until the cache is populated.', 'We can effectively handle the vast majority of hot spot with distributed cache and lock tables. Over time we made the following two improvements.', 'First, direct membership check of a user for an object and relation (i.e. ⟨ object # relation @ user ⟩ ) are usually handled by a single relation tuple lookup. However, occasionally a very popular object invite many concurrent check for different users, causing a hot spot on the storage server hosting rela- tion tuples for the object. To avoid these hot spots, we read and cache all relation tuples of ⟨ object # relation ⟩ for the hot object, trading read bandwidth for cacheability. We dynam- ically detect hot object to apply this method to by tracking the number of outstanding read on each object.', 'Second, indirect ACL check are frequently cancelled when the result of the parent ACL check is already deter- mined. This leaf the cache key unpopulated. While ea- ger cancellation reduces resource usage significantly, it neg- atively affect latency of concurrent request that are blocked by the lock table entry. To prevent this latency impact, we delay eager cancellation when there are waiter on the corre- sponding lock table entry.', '3.2.6 Performance Isolation', 'Performance isolation is indispensable for shared service targeting low latency and high availability. If Zanzibar or one of it client occasionally fails to provision enough re- source to handle an unexpected usage pattern, the following isolation mechanism ensure that performance problem are isolated to the problematic use case and do not adversely af- fect other clients.', 'First, to ensure proper allocation of CPU capacity, Zanz- ibar measure the cost of each RPC in term of generic cpu-seconds , a hardware-agnostic metric. Each client ha a global limit on maximum CPU usage per second; it RPCs will be throttled if it exceeds the limit and there is no spare capacity in the overall system.', 'Each Zanzibar server also limit the total number of out- standing RPCs to control it memory usage. Likewise it lim- it the number of oustanding RPCs per client.', 'Zanzibar further limit the maximum number of concur- rent read per (object, client) and per client on each Spanner server. This ensures that no single object or client can mo- nopolize a Spanner server.', 'Finally, we use different lock table key for request from different client to prevent any throttling that Spanner applies to one client from affecting other clients.', '3.2.7 Tail Latency Mitigation', 'Zanzibar’s distributed processing requires measure to ac- commodate slow tasks. For call to Spanner and to the Leop- ard index we rely on request hedging [16] (i.e. we send the same request to multiple servers, use whichever response come back first, and cancel the other requests). To reduce round-trip times, we try to place at least two replica of these backend service in every geographical region where we have Zanzibar servers. To avoid unnecessarily multiply- ing load, we first send one request and defer sending hedged request until the initial request is known to be slow.', 'To determine the appropriate hedging delay threshold, each server maintains a delay estimator that dynamically computes an N th percentile latency based on recent mea- surements. This mechanism allows u to limit the additional traffic incurred by hedging to a small fraction of total traffic.', 'Effective hedging requires the request to have similar costs. In the case of Zanzibar’s authorization checks, some check are inherently more time-consuming than others be- cause they require more work. Hedging check request would result in duplicating the most expensive workload and, ironically, worsening latency. Therefore we do not hedge request between Zanzibar servers, but rely on the pre- viously discussed sharding among multiple replica and on monitoring mechanism to detect and avoid slow servers.', 'Figure 3: Rate of Check Safe and Check Recent request over a 7-day period in December 2018.', 'Zanzibar ha been in production use for more than 5 years. Throughout that time, the number of client using Zanzibar and the load they place on Zanzibar have grown steadily. This section discus our experience operating Zanzibar a a globally distributed authorization system. Zanzibar manages more than 1,500 namespaces defined by hundred of client applications. The size of a namespace configuration file serf a a rough measure of the complex- ity of the access control policy implemented by that names- pace. These configuration file range from ten of line to thousand of lines, with the median near 500 lines. These namespaces contain more than 2 trillion relation tu- ples that occupy close to 100 terabytes. The number of tuples per namespace range over many order of magnitude, from ten to a trillion, with the median near 15,000. This data is fully replicated in more than 30 location around the world to maintain both proximity to user and high availability. Zanzibar serf more than 10 million client query per second (QPS). Over a sample 7-day period in December 2018, Check request peak at roughly 4.2M QPS, Read at 8.2M, Expand at 760K, and Write at 25K. Queries that read data are thus two order of magnitude more frequent than those that write data. Zanzibar distributes this load across more than 10,000 server organized in several dozen cluster around the world. The number of server per cluster range from fewer than 100 to more than 1,000, with the median near 500. Clusters are sized in proportion to load in their geographic regions.', 'We divide request into two category according to the re- quired data freshness, which can have a large impact on la- tency and availability of the requests. Specifically, Check, Read, and Expand request carry zookies to specify lower bound on evaluation timestamps. When a zookie timestamp is higher than that of the most recent data replicated to the region, the storage read require cross-region round trip to the leader replica to retrieve fresher data. As our storage', 'Figure 4: Latency of Check Safe response at different per- centile over a 7-day period in December 2018.', 'is configured with replication heartbeat with 8-second in- tervals, we divide our request into two categories: Safe re- quest have zookies more than 10 second old and can be served within the region most of time, while Recent request have zookies le than 10 second old and often require inter- region round trips. We report separate statistic for each.', 'Figure 3 show the rate of Check Safe and Check Recent request over 7 days. Both exhibit a diurnal cycle. The rate of Safe request is about two order of magnitude larger than that of Recent requests, which allows Zanzibar to serve the vast majority of ACL check locally.', 'Zanzibar’s latency budget is generally a small fraction of the few hundred of millisecond of total response time that it client must provide to be viable interactive services. Con- sider for example a client that performs authorization check on multiple document before it can show the result of a search on those documents.', 'We measure latency on the server side using live traffic because (1) latency is heavily influenced by our caching and de-duplication mechanism so that it is only realistically re- flected by live traffic, and (2) accurately measuring latency from client requires well-behaving clients. Provisioning of client job is outside of Zanzibar’s control and sometimes client job are overloaded.', 'Figure 4 show the latency of Check Safe response over 7 days. At the 50th, 95th, 99th, and 99.9th percentile it peak at roughly 3, 11, 20, and 93 msec, respectively. This performance meet our latency goal for an operation that is frequently in the critical path of user interactions.', 'Table 2 summarizes the latency distribution of Check, Read, Expand, and Write response over the same 7 days. As intended, the more frequently used Safe version of Check, Read, and Expand are significantly faster than the le fre- quently used Recent versions. Writes are the least frequently used of all the APIs, and the slowest because they always require distributed coordination among Spanner servers.', 'Latency in milliseconds, µ ( σ )', 'API 50%ile 95%ile 99%ile', 'Safe', 'Check 3 . 0 ( 0 . 091 ) 9 . 46 ( 0 . 3 ) 15 . 0 ( 1 . 19 )', 'Read 2 . 18 ( 0 . 031 ) 3 . 71 ( 0 . 094 ) 8 . 03 ( 3 . 28 )', 'Expand 4 . 27 ( 0 . 313 ) 8 . 84 ( 0 . 586 ) 34 . 1 ( 4 . 35 )', 'Recent', 'Check 2 . 86 ( 0 . 087 ) 60 . 0 ( 2 . 1 ) 76 . 3 ( 2 . 59 )', 'Read 2 . 21 ( 0 . 054 ) 40 . 1 ( 2 . 03 ) 86 . 2 ( 3 . 84 )', 'Expand 5 . 79 ( 0 . 224 ) 45 . 6 ( 3 . 44 ) 121 . 0 ( 2 . 38 )', 'Write 127 . 0 ( 3 . 65 ) 233 . 0 ( 23 . 0 ) 401 . 0 ( 133 . 0 )', 'Table 2: Mean and standard deviation of RPC response la- tency over a 7-day period in December 2018.', 'We define availability a the fraction of “qualified” RPCs the service answer successfully within latency thresholds: 5 second for a Safe request, and 15 second for a Recent re- quest a leader re-election in Spanner may take up to 10 sec- onds. For an RPC to be qualified, the request must be well- formed and have a deadline longer than the latency threshold. In addition, the client must stay within it resource quota. For these reasons, we cannot measure availability directly with live traffic, a our client sometimes send RPCs with short deadline or cancel their in-progress RPCs. Instead, we sample a small fraction of valid request from live traffic and replay them later with our own probers. When replaying the requests, we set the timeout to be longer than the avail- ability threshold. We also adjust the request zookie, if one is specified, so that the relative age of the zookie remains the same a when the request wa received in the live traffic. Finally, we run 3 probers per cluster and exclude outlier to eliminate false alarm caused by rare prober failures. To compute availability, we aggregate success ratio over 90-day window averaged across clusters. Figure 5 show Zanzibar’s availability a measured by these probers. Avail- ability ha remained above 99 . 999% over the past 3 year of operation at Google. In other words, for every quarter, Zanz- ibar ha le than 2 minute of global downtime and fewer than 13 minute when the global error ratio exceeds 10%.', 'Zanzibar server delegate check and read to each other based on consistent hashing, and both the caller and the callee side of the delegated operation cache the result to prevent hot spot (§3.2.5). At peak, Zanzibar handle 22 mil- lion internal “delegated” RPCs per second, split about evenly between read and checks. In-memory caching handle ap- proximately 200 million lookup per second at peak, 150 million from check and 50 million from reads. Caching for', 'Figure 5: Zanzibar’s availability over the past three year ha remained above 99 . 999% .', 'check ha a 10% hit rate on the delegate’s side, with an ad- ditional 12% saved by the lock table. Meanwhile, caching on the delegator’s side ha a 2% hit rate with an additional 3% from the lock table. While these hit rate appear low, they prevent 500K internal RPCs per second from creating hot spots. Delegated read see higher hit rate on the delegate’s side—24% on the cache and 9% on the lock table—but the delegator’s cache is hit le than 1% of the time. For super-hot groups, Zanzibar further optimizes by reading and caching the full set of member in advance—this happens for 0.1% of group but further prevents hot spots. This caching, along with aggressive pooling of read re- quests, allows Zanzibar to issue only 20 million read RPCs per second to Spanner. The median of these request read 1.5 row per RPC, but at the 99th percentile they each read close to 1 thousand rows. Zanzibar’s Spanner read take 0.5 msec at the median, and 2 msec at the 95th percentile. We find that 1% of Spanner reads, or 200K read per second, benefit from hedging. We note that Zanzibar us an instance of Spanner that run in- ternally to Google, not an instance of Cloud Spanner [6]. The Leopard index is performing 1.56M QPS at the me- dian, or 2.22M QPS at the 99th percentile, based on data ag- gregated over 7 days. Over the same 7 days, Leopard server respond in fewer than 150 µsec at the median, or under 1 msec at the 99th percentile. Leopard’s incremental layer dur- ing those 7 day writes roughly 500 index update per second at the median, and approximately 1.5K update per second at the 99th percentile.', 'Zanzibar ha evolved to meet the varied and heavy demand of a growing set of clients, including Google Calendar, Google Cloud, Google Drive, Google Maps, Google Photos, and YouTube. This section highlight lesson learned from this experience. One common theme ha been the importance of flexibility to accommodate difference between clients. For example:', '• Access control pattern vary widely: Over time we have added feature to support specific clients. For instance,', 'we added computed userset to allow inferring an object’s owner ID from the object ID prefix, which re- duce space requirement for client such a Drive and Photos that manage many private objects. Similarly, we added tuple to userset to represent object hierar- chy with only one relation tuple per hop. The bene- fit are both space reduction and flexibility—it allows client such a Cloud both to express ACL inheritance compactly and to change ACL inheritance rule without having to update large number of tuples. See §2.3.1.', '• Freshness requirement are often but not always loose: Clients often allow unspecified, moderate staleness dur- ing ACL evaluation, but sometimes require more pre- cisely specified freshness. We designed our zookie pro- tocol around this property so that we can serve most request from a default, already replicated snapshot, while allowing client to bound the staleness when needed. We also tuned the granularity of our snap- shot timestamps to match clients’ freshness require- ments. The resulting coarse timestamp quantum allow u to perform the majority of authorization check on a small number of snapshots, thus greatly reducing the frequency of database reads. See §3.2.1.', 'Another theme ha been the need to add performance opti- mizations to support client behavior observed in production. For example:', '• Request hedging is key to reducing tail latency: Clients that offer search capability to their users, such a Drive, often issue ten to hundred of authorization check to serve a single set of search results. We in- troduced hedging of Spanner and Leopard request to prevent an occasional slow operation from slowing the overall user interaction. See §3.2.7.', '• Hot-spot mitigation is critical for high availability: Some workload create hot spot in ACL data that can overwhelm the underlying database servers. A com- mon pattern is a burst of ACL check for an object that is indirectly referenced by the ACLs for many differ- ent objects. Specific instance arise from the search use case mentioned above, where the document in the search indirectly share ACLs for a large social or work group, and Cloud use case where many object indi- rectly share ACLs for the same object high in a hier- archy. Zanzibar handle most hot spot with general mechanism such a it distributed cache and lock ta- ble, but we have found the need to optimize specific us cases. For example, we added cache prefetching of all relation tuples for a hot object. We also delayed cancellation of secondary ACL check when there are concurrent request for the same ACL data. See §3.2.5.', '• Performance isolation is indispensable to protect against misbehaving clients: Even with hot-spot mit- igation measures, unexpected and sometimes unin-', 'tended client behavior could still overload our sys- tem or it underlying infrastructure. Examples include when client launch new feature that prove unexpect- edly popular or exercise Zanzibar in unintended ways. Over time we have added isolation safeguard to ensure that there are no cascading failure between client or between object of the same client. These safeguard include fine-grained cost accounting, quotas, and throt- tling. See §3.2.6.', 'Zanzibar is a planet-scale distributed ACL storage and eval- uation system. Many of it authorization concept have been explored previously within the domain of access control and social graphs, and it scaling challenge have been investi- gated within the field of distributed systems. Access control is a core part of multi-user operating sys- tems. Multics [23] support ACLs on segment and direc- tories. ACL entry consist of a principal identifier and a set of permission bits. In the first edition of UNIX [9], file flag indicate whether owner and non-owner can read or write the file. By the 4th edition, the permission bit had been expanded to read/write/execute bit for owner, group, and others. POSIX ACLs [4] add an arbitrary list of user and groups, each with up to 32 permission bits. VMS [7, 8] support ACL inheritance for file created within a direc- tory tree. Zanzibar’s data model support permissions, users, groups, and inheritance a found in the above systems. Taos [24, 10] support compound principal that incor- porate how an identity ha been transformed a it pass through a distributed system. For example, if user U logged into workstation W to access file server S , S would see re- quest authenticated a “ W for U ” rather than just U . This would allow one to write an ACL on a user’s e-mail that would be accessible only to the user, and only if being ac- cessed via the mail server. Abadi et al. discus in [11] a model of group-based ACLs with support for compound identities. Their notion of “blessings” are similar to Zanz- ibar tuples. However, Zanzibar adopts a unified represen- tation for ACLs and group using usersets, while they are separate concept in [11]. Role-based access control (RBAC), first proposed in [17], introduced the notion of role , which are similar to Zanzibar relations. Roles can inherit from each other and imply per- missions. A number of Zanzibar client have implemented RBAC policy on top of Zanzibar’s namespace configura- tion language. A discussion of ACL store in 2019 would be remiss with- out mentioning the Identity and Access Management (IAM) system offered commercially by Amazon [1], Google [5], Microsoft [2], and others. These system allow customer of those companies’ cloud product to configure flexible access control based on various feature such as: assigning user to', 'role or groups; domain-specific policy languages; and APIs that allow the creation and modification of ACLs. What all of these system have in common is unified ACL storage and an RPC-based API, a philosophy also core to Zanzibar’s de- sign. Google’s Cloud IAM system [5] is built a a layer on top of Zanzibar’s ACL storage and evaluation system.', 'TAO [13] is a distributed datastore for Facebook’s social graph. Several Zanzibar client also use Zanzibar to store their social graphs. Both Zanzibar and TAO provide au- thorization check to clients. Both are deployed a single- instance services, both operate at a large scale, and both are optimized for read-only operations. TAO offer eventual global consistency with asynchronous replication and best- effort read-after-write consistency with synchronous cache updates. In contrast, Zanzibar provides external consistency and snapshot read with bounded staleness, so that it respect causal ordering between ACL and content update and thus protects against the “new enemy” problem.', 'Lamport clock [22] provide partially ordered vector timestamps that can be used to determine the order of events. However, Lamport clock require explicit participation of all “processes”, where in Zanzibar’s use case some of the “processes” can be external client or even human users. In contrast, Zanzibar relies on it underlying database system, Spanner [15], to offer both external consistency and snapshot read with bounded staleness. In particular, Zanzibar build on Spanner’s TrueTime abstraction [15] to provide lineariz- able commit timestamps encoded a zookies.', 'At the same time, Zanzibar add a number of feature on top of those provided by Spanner. For one, the zookie proto- col doe not let client read or evaluate ACLs at an arbitrary snapshot. This restriction allows Zanzibar to choose a snap- shot that facilitates fast ACL evaluation. In addition, Zanz- ibar provides resilience to database hotspot (e.g. authoriza- tion check on a suddenly popular video) and safe pointer chasing despite potentially deep recursion (e.g. membership check on hierarchical groups).', 'The Chubby distributed lock service [14] offer reliable storage, linearizes writes, and provides access control, but it lack feature needed to support Zanzibar’s use cases. In particular, it doe not support high volume of data, effi- cient range reads, or read at a client-specified snapshot with bounded staleness. Its cache invalidation mechanism also limit it write throughput.', 'Finally, ZooKeeper offer a high-performance coordina- tion service [19] but also lack feature required by Zanz- ibar. Relative to Chubby, it can handle higher read and write rate with more relaxed cache consistency. However, it doe not provide external consistency for update across different node since it linearizability is on a per-node basis. It also doe not provide snapshot read with bounded staleness.', 'The Zanzibar authorization system unifies access control data and logic for Google. Its simple yet flexible data model and configuration language support a variety of access con- trol policy from both consumer and enterprise applications. Zanzibar’s external consistency model is one of it most salient features. It respect the ordering of user actions, yet at the same time allows authorization check to be evaluated at distributed location without global synchronization. Zanzibar employ other key technique to provide scal- ability, low latency, and high availability. For example, it evaluates deeply or widely nested group membership with Leopard, a specialized index for efficient computation of set operation with snapshot consistency. As another example, it combine a distributed cache with a mechanism to dedupli- cate in-flight requests. It thus mitigates hot spots, a critical production issue when serving data on top of normalized, consistent storage. These measure together result in a sys- tem that scale to trillion of access control rule and million of authorization request per second.', 'Many people have made technical contribution to Zanzibar. We thank previous and recent member of the development team, including Dan Barella, Miles Chaston, Daria Jung, Alex Mendes da Costa, Xin Pan, Scott Smith, Matthew Stef- fen, Riva Tropp, and Yuliya Zabiyaka. We also thank previ- ous and current member of the Site Reliability Engineering team, including Randall Bosetti, Hannes Eder, Robert Geis- berger, Tom Li, Massimo Maggi, Igor Oks, Aaron Peterson, and Andrea Yu. In addition, a number of people have helped to improve this paper. We received insightful comment from David Ba- con, Carolin G¨athke, Brad Krueger, Ari Shamash, Kai Shen, and Lawrence You. We are also grateful to Nadav Eiron and Royal Hansen for their support. Finally, we thank the anony- mous reviewer and our shepherd, Eric Eide, for their con- structive feedback.', '[1] Amazon Web Services Identity and Access Manage- ment. https://aws.amazon.com/iam/ . Accessed: 2019-04-16.', '[2] Azure Identity and Access Management. https: //www.microsoft.com/en-us/cloud-platform/ identity-management . Accessed: 2019-04-16.', '[3] Cache stampede. https://en.wikipedia.org/ wiki/Cache_stampede . Accessed: 2019-04-16.', '[4] DCE 1.1: Authentication and Security Services. http: //pubs.opengroup.org/onlinepubs/9668899 . Accessed: 2019-04-16.', '[5] Google Cloud Identity and Access Management. https://cloud.google.com/iam/ . Accessed: 2019-04-16.', '[6] Google Cloud Spanner. https://cloud.google. com/spanner/ . Accessed: 2019-04-16.', '[7] HP OpenVMS System Management Utilities Refer- ence Manual. https://support.hpe.com/hpsc/ doc/public/display?docId=emr_na-c04622366 . Accessed: 2019-04-16.', '[8] OpenVMS Guide to System Security. http: //www.itec.suny.edu/scsys/vms/ovmsdoc073/ V73/6346/6346pro_006.html#acl_details . Accessed: 2019-04-16.', '[9] Unix Manual. https://www.bell-labs.com/usr/ dmr/www/pdfs/man22.pdf . Accessed: 2019-04-16.', '[10] A', ', M., B', ', M., L', ', B.,', 'P', ', G. A calculus for access control in dis- tributed systems. ACM Trans. Program. Lang. Syst. 15 , 4 (Sept. 1993), 706–734.', '[11] A', ', M., B', ', M., P', ', H., S', ', A., S', ', A.,', 'T', ', A. Distributed au- thorization with distributed grammars. In Essays Ded- icated to Pierpaolo Degano on Programming Lan- guages with Applications to Biology and Security - Volume 9465 (New York, NY, USA, 2015), Springer- Verlag New York, Inc., pp. 10–26.', '[12] A', ', A., M', ', D., H', ', J., E', ', J., M', ', C., K', ', V., F', ', S., G', ', P., B', '-', ', L., H', ', J., P', ', R., K', ', L., S', ', A., M', ', A.,', 'L', '-A', ', K. Slicer: Auto-sharding for datacenter applications. In 12th USENIX Symposium on Operating Systems De- sign and Implementation (OSDI 16) (Savannah, GA, 2016), USENIX Association, pp. 739–753.', '[13] B', ', N., A', ', Z., C', ', G., C', ', P., D', ', P., D', ', H., F', ', J., G', '-', ', A., K', ', S., L', ', H., M', ', M., P', ', D., P', ', L., S', ', Y. J.,', 'V', ', V. TAO: Facebook’s distributed data store for the social graph. In Proceedings of the 2013 USENIX Annual Technical Conference (2013), USENIX ATC ’13, pp. 49–60.', '[14] B', ', M. The Chubby lock service for loosely- coupled distributed systems. In Proceedings of the', '7th Symposium on Operating Systems Design and Im- plementation (Berkeley, CA, USA, 2006), OSDI ’06, USENIX Association, pp. 335–350.', '[15] C', ', J. C., D', ', J., E', ', M., F', ', A., F', ', C., F', ', J. J., G', ', S., G', ', A., H', ', C., H', ', P., H', ', W., K', ', S., K', ', E., L', ', H., L', ', A., M', ', S., M', ', D., N', ', D., Q', ', S., R', ', R., R', ', L., S', ', Y., S', '-', ', M., T', ', C., W', ', R.,', 'W', '-', ', D. Spanner: Google’s globally-distributed database. In Proceedings of the 10th USENIX Confer- ence on Operating Systems Design and Implementation (2012), OSDI ’12, pp. 251–264.', '[16] D', ', J.,', 'B', ', L. A. The tail at scale. Communications of the ACM 56 , 2 (Feb. 2013), 74–80.', '[17] F', ', D.,', 'K', ', R. Role-based access control. In In 15th NIST-NCSC National Computer Se- curity Conference (1992), pp. 554–563.', '[18] G', ', D. K. Information Storage in a Decentral- ized Computer System . PhD thesis, Stanford, CA, USA, 1981. AAI8124072.', '[19] H', ', P., K', ', M., J', ', F. P.,', 'R', ', B. Zookeeper: Wait-free coordination for internet-scale systems. In Proceedings of the 2010 USENIX Annual Technical Conference (Berkeley, CA, USA, 2010), USENIX ATC ’10, USENIX Association.', '[20] K', ', D., L', ', E., L', ', T., P', '-', ', R., L', ', M.,', 'L', ', D. Consistent hashing and random trees: Distributed caching proto- col for relieving hot spot on the world wide web. In Proceedings of the Twenty-ninth Annual ACM Sympo- sium on Theory of Computing (New York, NY, USA, 1997), STOC ’97, ACM, pp. 654–663.', '[21] K', ', H. T.,', 'R', ', J. T. On opti- mistic method for concurrency control. ACM Trans. Database Syst. 6 , 2 (June 1981), 213–226.', '[22] L', ', L. Time, clocks, and the ordering of event in a distributed system. Commun. ACM 21 , 7 (July 1978), 558–565.', '[23] S', ', J. H. Protection and control of infor- mation sharing in Multics. In Proceedings of the Fourth ACM Symposium on Operating System Princi- ples (New York, NY, USA, 1973), SOSP ’73, ACM.', '[24] W', ', E., A', ', M., B', ', M.,', 'L', ', B. Authentication in the Taos operating system. In Proceedings of the Fourteenth ACM Sympo- sium on Operating Systems Principles (New York, NY, USA, 1993), SOSP ’93, ACM, pp. 256–269.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_paras = []\n",
    "for i in range(len(paragraphs)):\n",
    "    if paragraphs[i] != \"\":\n",
    "        lemmatized_paras.append(LemmatizerHelper.lemmatize(paragraphs[i]))\n",
    "print(lemmatized_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "# word_list = []\n",
    "# for paragraph in stemmed_paras:\n",
    "#     if paragraph != '':\n",
    "#         filtered_sentence = remove_stopwords(paragraph)\n",
    "#         temp = filtered_sentence.split(\" \")\n",
    "#         word_list.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# min_count = 3\n",
    "# size = 100\n",
    "# window = 4\n",
    " \n",
    "# model = Word2Vec(word_list, min_count=min_count, vector_size=size, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_terms = list(model.wv.index_to_key)\n",
    "# key_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_key_terms = []\n",
    "# for i in key_terms:\n",
    "#     if len(i) >= 3:\n",
    "#         new_key_terms.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 466kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 83.1kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.40MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 762kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 52.2kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 8.16MB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [01:37<00:00, 4.51MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 20.3kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 71.7kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 267kB/s] \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 116kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 3.12MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 197kB/s]  \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 99.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['opengroup', 'terabytes', 'compute', 'policy', 'quota', 'usersets', 'caching', 'users', 'metadata', 'data', 'public', '401', 'hashing', 'cloud', 'authorized', 'hosting', 'permis', 'computing', 'google', 'internet', 'azure', 'safeguard', 'tor', 'protection', 'aws', 'acl', 'storage', 'cache', 'http', 'autho', 'decentral', 'aclservers', 'database', 'datastore', 'permission', 'https', 'permissions', 'access', 'api', 'databases', 'apis', 'acls', 'cacheability', 'authorize', 'acl_details', 'authentication', 'privacy', 'authorization', 'security', 'authorizations']\n",
      "<class 'list'>\n",
      "opengroup,terabytes,compute,policy,quota,usersets,caching,users,metadata,data,public,401,hashing,cloud,authorized,hosting,permis,computing,google,internet,azure,safeguard,tor,protection,aws,acl,storage,cache,http,autho,decentral,aclservers,database,datastore,permission,https,permissions,access,api,databases,apis,acls,cacheability,authorize,acl_details,authentication,privacy,authorization,security,authorizations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opengroup,terabytes,compute,policy,quota,usersets,caching,users,metadata,data,public,401,hashing,cloud,authorized,hosting,permis,computing,google,internet,azure,safeguard,tor,protection,aws,acl,storage,cache,http,autho,decentral,aclservers,database,datastore,permission,https,permissions,access,api,databases,apis,acls,cacheability,authorize,acl_details,authentication,privacy,authorization,security,authorizations'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class KWE:\n",
    "    # text=\"\"\"Determining whether online users are authorized to access digital objects is central to preserving privacy. This pa- per presents the design, implementation, and deployment of Zanzibar, a global system for storing and evaluating ac- cess control lists. Zanzibar provides a uniform data model and configuration language for expressing a wide range of access control policies from hundreds of client services at Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Its authorization decisions respect causal or- dering of user actions and thus provide external consistency amid changes to access control lists and object contents. Zanzibar scales to trillions of access control lists and millions of authorization requests per second to support services used by billions of people. It has maintained 95th-percentile la- tency of less than 10 milliseconds and availability of greater than 99.999% over 3 years of production use.\"\"\"\n",
    "\n",
    "    def __init__(self,t):\n",
    "        self.text=t\n",
    "\n",
    "    def keywordExtract(self):\n",
    "        n_gram_range = (1, 1)\n",
    "        stop_words = \"english\"\n",
    "        count = CountVectorizer(ngram_range=n_gram_range,\n",
    "                                stop_words=stop_words).fit(self.text)\n",
    "\n",
    "        candidates = count.get_feature_names_out()\n",
    "\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        doc_embedding = model.encode(self.text)\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        top_n = 50\n",
    "        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "        keywords = [candidates[index] for index in distances.argsort()[0]][-top_n:]\n",
    "\n",
    "        print(keywords)\n",
    "\n",
    "        print(type(keywords))\n",
    "\n",
    "        s = \",\".join(keywords)\n",
    "\n",
    "        print(s)\n",
    "        return s\n",
    "    # keywrdextract(data)\n",
    "# if _name_ == \"_main_\":\n",
    "c = KWE(lemmatized_paras)\n",
    "c.keywordExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mindmaps')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dc3d65545c52fc3480745454bf9ad36b9e9094198317f42229a0b71dd4fc340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
